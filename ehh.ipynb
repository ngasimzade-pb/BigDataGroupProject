{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T06:00:56.701529Z",
     "start_time": "2024-12-15T06:00:54.946649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: apt-get\n",
      "zsh:1: command not found: wget\n",
      "tar: Error opening archive: Failed to open 'spark-3.5.0-bin-hadoop3.tgz'\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/path/to/spark-3.5.0-bin-hadoop3/./bin/spark-submit'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 28\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlinalg\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Vectors\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# Start Spark session\u001B[39;00m\n\u001B[0;32m---> 28\u001B[0m sp \u001B[38;5;241m=\u001B[39m (\u001B[43mSparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuilder\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mappName\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mAir Quality Data Classification\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mspark.sql.crossJoin.enabled\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrue\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mspark.serializer\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43morg.apache.spark.serializer.KryoSerializer\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     34\u001B[0m sp\u001B[38;5;241m.\u001B[39msparkContext\u001B[38;5;241m.\u001B[39msetLogLevel(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mINFO\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Load Dataset from Local File\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/BigDataGroupProject/.venv/lib/python3.9/site-packages/pyspark/sql/session.py:497\u001B[0m, in \u001B[0;36mSparkSession.Builder.getOrCreate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    495\u001B[0m     sparkConf\u001B[38;5;241m.\u001B[39mset(key, value)\n\u001B[1;32m    496\u001B[0m \u001B[38;5;66;03m# This SparkContext may be an existing one.\u001B[39;00m\n\u001B[0;32m--> 497\u001B[0m sc \u001B[38;5;241m=\u001B[39m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msparkConf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    498\u001B[0m \u001B[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001B[39;00m\n\u001B[1;32m    499\u001B[0m \u001B[38;5;66;03m# by all sessions.\u001B[39;00m\n\u001B[1;32m    500\u001B[0m session \u001B[38;5;241m=\u001B[39m SparkSession(sc, options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_options)\n",
      "File \u001B[0;32m~/PycharmProjects/BigDataGroupProject/.venv/lib/python3.9/site-packages/pyspark/context.py:515\u001B[0m, in \u001B[0;36mSparkContext.getOrCreate\u001B[0;34m(cls, conf)\u001B[0m\n\u001B[1;32m    513\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m    514\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 515\u001B[0m         \u001B[43mSparkContext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mSparkConf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    516\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    517\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\n",
      "File \u001B[0;32m~/PycharmProjects/BigDataGroupProject/.venv/lib/python3.9/site-packages/pyspark/context.py:201\u001B[0m, in \u001B[0;36mSparkContext.__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001B[0m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m gateway \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m gateway\u001B[38;5;241m.\u001B[39mgateway_parameters\u001B[38;5;241m.\u001B[39mauth_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    197\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    198\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is not allowed as it is a security risk.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    199\u001B[0m     )\n\u001B[0;32m--> 201\u001B[0m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ensure_initialized\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgateway\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgateway\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_init(\n\u001B[1;32m    204\u001B[0m         master,\n\u001B[1;32m    205\u001B[0m         appName,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    215\u001B[0m         memory_profiler_cls,\n\u001B[1;32m    216\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/BigDataGroupProject/.venv/lib/python3.9/site-packages/pyspark/context.py:436\u001B[0m, in \u001B[0;36mSparkContext._ensure_initialized\u001B[0;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[1;32m    434\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m    435\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_gateway:\n\u001B[0;32m--> 436\u001B[0m         SparkContext\u001B[38;5;241m.\u001B[39m_gateway \u001B[38;5;241m=\u001B[39m gateway \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mlaunch_gateway\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    437\u001B[0m         SparkContext\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;241m=\u001B[39m SparkContext\u001B[38;5;241m.\u001B[39m_gateway\u001B[38;5;241m.\u001B[39mjvm\n\u001B[1;32m    439\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m instance:\n",
      "File \u001B[0;32m~/PycharmProjects/BigDataGroupProject/.venv/lib/python3.9/site-packages/pyspark/java_gateway.py:97\u001B[0m, in \u001B[0;36mlaunch_gateway\u001B[0;34m(conf, popen_kwargs)\u001B[0m\n\u001B[1;32m     94\u001B[0m         signal\u001B[38;5;241m.\u001B[39msignal(signal\u001B[38;5;241m.\u001B[39mSIGINT, signal\u001B[38;5;241m.\u001B[39mSIG_IGN)\n\u001B[1;32m     96\u001B[0m     popen_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpreexec_fn\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m preexec_func\n\u001B[0;32m---> 97\u001B[0m     proc \u001B[38;5;241m=\u001B[39m \u001B[43mPopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpopen_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     99\u001B[0m     \u001B[38;5;66;03m# preexec_fn not supported on Windows\u001B[39;00m\n\u001B[1;32m    100\u001B[0m     proc \u001B[38;5;241m=\u001B[39m Popen(command, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpopen_kwargs)\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/subprocess.py:951\u001B[0m, in \u001B[0;36mPopen.__init__\u001B[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001B[0m\n\u001B[1;32m    947\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext_mode:\n\u001B[1;32m    948\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstderr \u001B[38;5;241m=\u001B[39m io\u001B[38;5;241m.\u001B[39mTextIOWrapper(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstderr,\n\u001B[1;32m    949\u001B[0m                     encoding\u001B[38;5;241m=\u001B[39mencoding, errors\u001B[38;5;241m=\u001B[39merrors)\n\u001B[0;32m--> 951\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execute_child\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexecutable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreexec_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclose_fds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    952\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mpass_fds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcwd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    953\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mstartupinfo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreationflags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshell\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    954\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mp2cread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp2cwrite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    955\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mc2pread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc2pwrite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    956\u001B[0m \u001B[43m                        \u001B[49m\u001B[43merrread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrwrite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    957\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mrestore_signals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    958\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mgid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mumask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    959\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mstart_new_session\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    960\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[1;32m    961\u001B[0m     \u001B[38;5;66;03m# Cleanup if the child failed starting.\u001B[39;00m\n\u001B[1;32m    962\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mfilter\u001B[39m(\u001B[38;5;28;01mNone\u001B[39;00m, (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstdin, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstdout, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstderr)):\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/subprocess.py:1821\u001B[0m, in \u001B[0;36mPopen._execute_child\u001B[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001B[0m\n\u001B[1;32m   1819\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m errno_num \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1820\u001B[0m         err_msg \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mstrerror(errno_num)\n\u001B[0;32m-> 1821\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001B[1;32m   1822\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m child_exception_type(err_msg)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/path/to/spark-3.5.0-bin-hadoop3/./bin/spark-submit'"
     ]
    }
   ],
   "source": [
    "# Install and setup Spark\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q http://apache.osuosl.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
    "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set environment variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ['SPARK_HOME'] = '/path/to/spark-3.5.0-bin-hadoop3'\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, Row\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, StopWordsRemover, RegexTokenizer, CountVectorizer, Word2Vec, NGram\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import VectorUDT\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Start Spark session\n",
    "sp = (SparkSession.builder\n",
    "      .appName('Air Quality Data Classification')\n",
    "      .config(\"spark.sql.crossJoin.enabled\", \"true\")\n",
    "      .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "      .getOrCreate())\n",
    "\n",
    "sp.sparkContext.setLogLevel('INFO')\n",
    "\n",
    "# Load Dataset from Local File\n",
    "def load_csv_to_spark(file_path: str):\n",
    "    \"\"\"\n",
    "    Load a CSV file into a Spark DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - Spark DataFrame\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Loading data from {file_path}\")\n",
    "        pandas_df = pd.read_csv(file_path, sep=';', header=0)\n",
    "        spark_df = sp.createDataFrame(pandas_df)\n",
    "        return spark_df\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"File {file_path} not found!\")\n",
    "\n",
    "# Specify file path\n",
    "file_path = \"./AirQualityUCI.csv\"\n",
    "data_df = load_csv_to_spark(file_path)\n",
    "\n",
    "# Display Schema and Initial Data\n",
    "data_df.printSchema()\n",
    "data_df.show(5)\n",
    "\n",
    "# Tokenization and Stop Words Removal\n",
    "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\", pattern=\"\\\\W\")\n",
    "wordsData = tokenizer.transform(data_df)\n",
    "\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "wordsData = stopwords_remover.transform(wordsData)\n",
    "\n",
    "# TF-IDF Features\n",
    "hashTF = HashingTF(inputCol=\"filtered_tokens\", outputCol=\"tfFeatures\")\n",
    "tfData = hashTF.transform(wordsData)\n",
    "\n",
    "idf = IDF(inputCol=\"tfFeatures\", outputCol=\"tfidfFeatures\")\n",
    "idfModel = idf.fit(tfData)\n",
    "tfidfData = idfModel.transform(tfData)\n",
    "\n",
    "# Bag of Words (BOW) Features\n",
    "count_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"BOWFeatures\")\n",
    "bowData = count_vectorizer.fit(wordsData).transform(wordsData)\n",
    "\n",
    "# N-Gram Features\n",
    "n_value = 3\n",
    "ngram = NGram(n=n_value, inputCol=\"filtered_tokens\", outputCol=\"ngram_features\")\n",
    "ngramData = ngram.transform(wordsData)\n",
    "\n",
    "# Model Evaluation Function\n",
    "def evaluate_model(predictions, label_col=\"class_label\", prediction_col=\"prediction\"):\n",
    "    evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=prediction_col, metricName=\"accuracy\")\n",
    "    accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    evaluator_f1 = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=prediction_col, metricName=\"f1\")\n",
    "    f1 = evaluator_f1.evaluate(predictions)\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "\n",
    "    evaluator_recall = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=prediction_col, metricName=\"weightedRecall\")\n",
    "    recall = evaluator_recall.evaluate(predictions)\n",
    "    print(f\"Recall: {recall}\")\n",
    "\n",
    "    evaluator_precision = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=prediction_col, metricName=\"weightedPrecision\")\n",
    "    precision = evaluator_precision.evaluate(predictions)\n",
    "    print(f\"Precision: {precision}\")\n",
    "\n",
    "# Logistic Regression Model\n",
    "lr = LogisticRegression(regParam=0.1, labelCol=\"class_label\", featuresCol=\"tfidfFeatures\")\n",
    "lr_pipeline = Pipeline(stages=[lr])\n",
    "lr_model = lr_pipeline.fit(tfidfData)\n",
    "lr_predictions = lr_model.transform(tfidfData)\n",
    "evaluate_model(lr_predictions)\n",
    "\n",
    "# Decision Tree Model\n",
    "dt = DecisionTreeClassifier(labelCol=\"class_label\", featuresCol=\"bow\", maxDepth=7)\n",
    "dt_pipeline = Pipeline(stages=[dt])\n",
    "dt_model = dt_pipeline.fit(tfidfData)\n",
    "dt_predictions = dt_model.transform(tfidfData)\n",
    "evaluate_model(dt_predictions)\n",
    "\n",
    "# Random Forest Model\n",
    "rf = RandomForestClassifier(labelCol=\"class_label\", featuresCol=\"tfidfFeatures\", numTrees=25)\n",
    "rf_pipeline = Pipeline(stages=[rf])\n",
    "rf_model = rf_pipeline.fit(tfidfData)\n",
    "rf_predictions = rf_model.transform(tfidfData)\n",
    "evaluate_model(rf_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25daf47-d8b8-4eaf-be62-24318caa29f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463a3a8f-9024-43d2-82f6-fb1db96b98b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
