{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-15T06:00:56.701529Z",
     "start_time": "2024-12-15T06:00:54.946649Z"
    }
   },
   "source": [
    "# Install and setup Spark\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q http://apache.osuosl.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
    "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set environment variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
    "\n",
    "# Initialize Spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, Row\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, StopWordsRemover, RegexTokenizer, CountVectorizer, Word2Vec, NGram\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import VectorUDT\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Start Spark session\n",
    "sp = (SparkSession.builder\n",
    "      .appName('Air Quality Data Classification')\n",
    "      .config(\"spark.sql.crossJoin.enabled\", \"true\")\n",
    "      .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "      .getOrCreate())\n",
    "\n",
    "sp.sparkContext.setLogLevel('INFO')\n",
    "\n",
    "# Load Dataset from Local File\n",
    "def load_csv_to_spark(file_path: str):\n",
    "    \"\"\"\n",
    "    Load a CSV file into a Spark DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - Spark DataFrame\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Loading data from {file_path}\")\n",
    "        pandas_df = pd.read_csv(file_path, sep=';', header=0)\n",
    "        spark_df = sp.createDataFrame(pandas_df)\n",
    "        return spark_df\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"File {file_path} not found!\")\n",
    "\n",
    "# Specify file path\n",
    "file_path = \"./AirQualityUCI.csv\"\n",
    "data_df = load_csv_to_spark(file_path)\n",
    "\n",
    "# Display Schema and Initial Data\n",
    "data_df.printSchema()\n",
    "data_df.show(5)\n",
    "\n",
    "# Tokenization and Stop Words Removal\n",
    "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\", pattern=\"\\\\W\")\n",
    "wordsData = tokenizer.transform(data_df)\n",
    "\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "wordsData = stopwords_remover.transform(wordsData)\n",
    "\n",
    "# TF-IDF Features\n",
    "hashTF = HashingTF(inputCol=\"filtered_tokens\", outputCol=\"tfFeatures\")\n",
    "tfData = hashTF.transform(wordsData)\n",
    "\n",
    "idf = IDF(inputCol=\"tfFeatures\", outputCol=\"tfidfFeatures\")\n",
    "idfModel = idf.fit(tfData)\n",
    "tfidfData = idfModel.transform(tfData)\n",
    "\n",
    "# Bag of Words (BOW) Features\n",
    "count_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"BOWFeatures\")\n",
    "bowData = count_vectorizer.fit(wordsData).transform(wordsData)\n",
    "\n",
    "# N-Gram Features\n",
    "n_value = 3\n",
    "ngram = NGram(n=n_value, inputCol=\"filtered_tokens\", outputCol=\"ngram_features\")\n",
    "ngramData = ngram.transform(wordsData)\n",
    "\n",
    "# Word2Vec Features\n",
    "word2vec = Word2Vec(vectorSize=100, minCount=5, inputCol=\"filtered_tokens\", outputCol=\"Word2VecFeatures\")\n",
    "word2vec_model = word2vec.fit(wordsData)\n",
    "word2vecData = word2vec_model.transform(wordsData)\n",
    "\n",
    "# Model Evaluation Function\n",
    "def evaluate_model(predictions, label_col=\"class_label\", prediction_col=\"prediction\"):\n",
    "    evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=prediction_col, metricName=\"accuracy\")\n",
    "    accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    evaluator_f1 = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=prediction_col, metricName=\"f1\")\n",
    "    f1 = evaluator_f1.evaluate(predictions)\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "\n",
    "    evaluator_recall = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=prediction_col, metricName=\"weightedRecall\")\n",
    "    recall = evaluator_recall.evaluate(predictions)\n",
    "    print(f\"Recall: {recall}\")\n",
    "\n",
    "    evaluator_precision = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=prediction_col, metricName=\"weightedPrecision\")\n",
    "    precision = evaluator_precision.evaluate(predictions)\n",
    "    print(f\"Precision: {precision}\")\n",
    "\n",
    "# Logistic Regression Model\n",
    "lr = LogisticRegression(regParam=0.1, labelCol=\"class_label\", featuresCol=\"tfidfFeatures\")\n",
    "lr_pipeline = Pipeline(stages=[lr])\n",
    "lr_model = lr_pipeline.fit(tfidfData)\n",
    "lr_predictions = lr_model.transform(tfidfData)\n",
    "evaluate_model(lr_predictions)\n",
    "\n",
    "# Decision Tree Model\n",
    "dt = DecisionTreeClassifier(labelCol=\"class_label\", featuresCol=\"tfidfFeatures\", maxDepth=7)\n",
    "dt_pipeline = Pipeline(stages=[dt])\n",
    "dt_model = dt_pipeline.fit(tfidfData)\n",
    "dt_predictions = dt_model.transform(tfidfData)\n",
    "evaluate_model(dt_predictions)\n",
    "\n",
    "# Random Forest Model\n",
    "rf = RandomForestClassifier(labelCol=\"class_label\", featuresCol=\"tfidfFeatures\", numTrees=25)\n",
    "rf_pipeline = Pipeline(stages=[rf])\n",
    "rf_model = rf_pipeline.fit(tfidfData)\n",
    "rf_predictions = rf_model.transform(tfidfData)\n",
    "evaluate_model(rf_predictions)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: apt-get\r\n",
      "zsh:1: command not found: wget\r\n",
      "tar: Error opening archive: Failed to open 'spark-3.5.0-bin-hadoop3.tgz'\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Unable to find py4j in /content/spark-3.5.0-bin-hadoop3/python, your SPARK_HOME may not be configured correctly",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[0;32m~/PycharmProjects/BigDataGroupProject/.venv/lib/python3.9/site-packages/findspark.py:159\u001B[0m, in \u001B[0;36minit\u001B[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001B[0m\n\u001B[1;32m    158\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 159\u001B[0m     py4j \u001B[38;5;241m=\u001B[39m \u001B[43mglob\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mspark_python\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlib\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpy4j-*.zip\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m:\n",
      "\u001B[0;31mIndexError\u001B[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 14\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Initialize Spark\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mfindspark\u001B[39;00m\n\u001B[0;32m---> 14\u001B[0m \u001B[43mfindspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minit\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# Import required libraries\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/BigDataGroupProject/.venv/lib/python3.9/site-packages/findspark.py:161\u001B[0m, in \u001B[0;36minit\u001B[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001B[0m\n\u001B[1;32m    159\u001B[0m         py4j \u001B[38;5;241m=\u001B[39m glob(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(spark_python, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlib\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpy4j-*.zip\u001B[39m\u001B[38;5;124m\"\u001B[39m))[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    160\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m:\n\u001B[0;32m--> 161\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n\u001B[1;32m    162\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to find py4j in \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, your SPARK_HOME may not be configured correctly\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    163\u001B[0m                 spark_python\n\u001B[1;32m    164\u001B[0m             )\n\u001B[1;32m    165\u001B[0m         )\n\u001B[1;32m    166\u001B[0m     sys\u001B[38;5;241m.\u001B[39mpath[:\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m sys_path \u001B[38;5;241m=\u001B[39m [spark_python, py4j]\n\u001B[1;32m    167\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    168\u001B[0m     \u001B[38;5;66;03m# already imported, no need to patch sys.path\u001B[39;00m\n",
      "\u001B[0;31mException\u001B[0m: Unable to find py4j in /content/spark-3.5.0-bin-hadoop3/python, your SPARK_HOME may not be configured correctly"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
